services:
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> parent of 727170a (Merge pull request #1 from DuyNe07/feature/add-airflow)
    version-store: # version store for nessie
        image: postgres
        hostname: version-store
        container_name: version-store
        environment:
            - POSTGRES_PASSWORD=admin123
            - POSTGRES_USER=admin
            - POSTGRES_DB=catalog_nessie
        ports:
            - "5433:5432"
        volumes:
            - ./volumes/catalog_nessie_db:/var/lib/postgresql/data
        networks:
            - datalake
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -d catalog_nessie -U admin"]
            interval: 5s
            timeout: 5s
            retries: 5
<<<<<<< HEAD
=======
  version-store:
    image: postgres
    hostname: version-store
    container_name: version-store
    environment:
      - POSTGRES_PASSWORD=admin123
      - POSTGRES_USER=admin
      - POSTGRES_DB=catalog_nessie
    ports:
      - '5433:5432'
    volumes:
      - ./volumes/catalog_nessie_db:/var/lib/postgresql/data
    networks:
      - datalake
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d catalog_nessie -U admin"]
      interval: 5s
      timeout: 5s
      retries: 5
>>>>>>> feature/add-airflow
=======
>>>>>>> parent of 727170a (Merge pull request #1 from DuyNe07/feature/add-airflow)

    nessie: # catalog for iceberg
        image: ghcr.io/projectnessie/nessie
        container_name: nessie
        hostname: nessie
        environment:
            - nessie.version.store.type=JDBC2
            - nessie.version.store.persist.jdbc.datasource=postgresql
            - quarkus.datasource.postgresql.jdbc.url=jdbc:postgresql://version-store:5432/catalog_nessie
            - quarkus.datasource.postgresql.username=admin
            - quarkus.datasource.postgresql.password=admin123
            - nessie.catalog.default-warehouse=warehouse
            - nessie.catalog.warehouses.warehouse.location=s3://datalake/
            - nessie.catalog.service.s3.default-options.endpoint=http://minio:9000/
            - nessie.catalog.service.s3.default-options.access-key=urn:nessie-secret:quarkus:nessie.catalog.secrets.access-key
            - nessie.catalog.secrets.access-key.name=admin
            - nessie.catalog.secrets.access-key.secret=admin123
            - nessie.catalog.service.s3.default-options.region=us-east-1
            - nessie.server.authentication.enabled=false
        ports:
            - "19120:19120"
        volumes:
            - ./volumes/nessie:/data
            - ./nessie/conf/init-nessie.sh:/usr/bin/init-nessie.sh
            - ./nessie/bin/nessie-tools.sh:/usr/bin/nessie-tools
        networks:
            - datalake
        depends_on:
            - version-store
            - minio

    trino: # query engine
        image: "trinodb/trino:latest"
        container_name: trino
        hostname: trino
        environment:
            - AWS_ACCESS_KEY_ID=admin
            - AWS_SECRET_ACCESS_KEY=admin123
            - AWS_REGION=us-east-1
        ports:
            - "8060:8060"
        volumes:
            - ./trino/etc:/etc/trino
        networks:
            - datalake
        depends_on:
            - nessie

<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> parent of 727170a (Merge pull request #1 from DuyNe07/feature/add-airflow)
    spark:
        image: spark
        container_name: spark
        build: spark/
        environment:
            - AWS_ACCESS_KEY_ID=admin
            - AWS_SECRET_ACCESS_KEY=admin123
            - AWS_REGION=us-east-1
            - PYTHONPATH=/nessie:/src
            - PYSPARK_PYTHON=/usr/bin/python3
            - PYSPARK_DRIVER_PYTHON=/usr/bin/python3
            - NESSIE_API_URL=http://nessie:19120/api/v1
            - NESSIE_USER=duy-ne
        volumes:
            - ./src:/src
            - ./nessie:/nessie
            - ./nessie/conf/init-nessie.sh:/usr/bin/init-nessie.sh
            # - ./spark/scripts/entrypoint.sh:/usr/bin/entrypoint.sh
            - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
        ports:
            - 8888:8888 # Spark notebook port
            - 7077:7077 # Spark port
            - 8061:8061 # Spark master web ui port
            - 8062:8062 # Spark worker web ui port
            - 10000:10000 # Thrift ODBC/JDBC port
            - 10009:10009 # Kyuubi JDBC port
            - 18080:18080 # Spark history web ui port
            - 9083:9083 # Metastore thrift
            # - 10099:10099 # Kyuubi web ui port
        networks:
            - datalake
<<<<<<< HEAD
=======
  spark:
    image: spark-iceberg
    container_name: spark
    build: spark/
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=admin123
      - AWS_REGION=us-east-1
    volumes:
      - ./src:/src
      - ./spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    ports:
      - 8888:8888
      - 7077:7077
      - 8061:8061
      - 8062:8062
      - 10000:10000
      - 10009:10009
      - 18080:18080
      - 9083:9083
    networks:
      - datalake
      
  minio: # Storage
      hostname: minio
      image: 'minio/minio'
      container_name: minio
      ports:
        - '9000:9000'
        - '9001:9001'
      volumes:
        - ./volumes/minio:/data
      environment:
        - MINIO_ROOT_USER=admin
        - MINIO_ROOT_PASSWORD=admin123
      command: server /data --console-address ":9001"
      networks:
        - datalake
      healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
        interval: 5s
        timeout: 5s
        retries: 5
>>>>>>> feature/add-airflow
=======
>>>>>>> parent of 727170a (Merge pull request #1 from DuyNe07/feature/add-airflow)

    minio: # Storage
        hostname: minio
        image: "minio/minio"
        container_name: minio
        ports:
            - "9000:9000"
            - "9001:9001"
        volumes:
            - ./volumes/minio:/data
        environment:
            - MINIO_ROOT_USER=admin
            - MINIO_ROOT_PASSWORD=admin123
        command: server /data --console-address ":9001"
        networks:
            - datalake
        healthcheck:
            test:
                ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 5s
            timeout: 5s
            retries: 5

    mc: # minio client for creating default bucket
        image: minio/mc
        container_name: mc
        hostname: mc
        environment:
            - AWS_ACCESS_KEY_ID=admin
            - AWS_SECRET_ACCESS_KEY=admin123
            - AWS_REGION=us-east-1
        networks:
            - datalake
        volumes:
            - ./minio/init_minio.sh:/init_minio.sh
        entrypoint: ["/bin/sh", "-c", "/init_minio.sh"]
        depends_on:
            minio:
                condition: service_healthy

    cube:
        image: cubejs/cube
        container_name: cube
        hostname: cube
        volumes:
            - ./cube:/cube/conf
        ports:
            - 3000:3000
            - 4000:4000
            - 3245:3245
        networks:
            - datalake
        depends_on:
            - trino

<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> parent of 727170a (Merge pull request #1 from DuyNe07/feature/add-airflow)
    metabase:
        container_name: metabase
        depends_on:
            - cube
            - trino
        hostname: metabase
        build:
            context: ./metabase
        volumes:
            - ./metabase:/metabase-data
            - ./metabase/metabase-settings.yml:/metabase-settings.yml
        ports:
            - 3030:3030
        environment:
            - MB_DB_FILE=/metabase-data/metabase.db
            - MB_DB_TYPE=h2
            - MB_JETTY_PORT=3030
            - MB_ENCRYPTION_SECRET_KEY=thisisasecretkey!!
        networks:
            - datalake
<<<<<<< HEAD
=======
  airflow-init:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-init
    hostname: airflow-init
    depends_on:
      airflow-postgres:
        condition: service_healthy
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./src:/src
    networks:
      - datalake
    command: bash -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com"

  airflow-webserver:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-webserver
    hostname: airflow-webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      spark:
        condition: service_started
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=admin123
      - AWS_REGION=us-east-1
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark:7077
      - AIRFLOW__WEBSERVER__WEB_SERVER_PORT=9090
    ports:
      - 9090:9090
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./src:/src
    networks:
      - datalake
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:9090/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-scheduler
    hostname: airflow-scheduler
    depends_on:
      airflow-webserver:
        condition: service_healthy
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=admin123
      - AWS_REGION=us-east-1
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark:7077
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./src:/src
    networks:
      - datalake
    command: scheduler
>>>>>>> feature/add-airflow
=======
>>>>>>> parent of 727170a (Merge pull request #1 from DuyNe07/feature/add-airflow)

networks:
    datalake:
        driver: bridge
