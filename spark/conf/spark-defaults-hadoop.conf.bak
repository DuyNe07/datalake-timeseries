#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Spark Configuration
spark.driver.memory=16g
spark.executor.memory=16g
spark.executor.cores=16
spark.driver.memoryOverhead=8g
spark.executor.memoryOverhead=8g
spark.sql.adaptive.enabled=true
spark.sql.files.maxPartitionBytes=134217728
spark.sql.shuffle.partitions=200

# Kích hoạt chỉ tiện ích mở rộng Iceberg (không có Nessie)
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# Cấu hình ICEBERG
spark.sql.iceberg.handle-timestamp-without-timezone=true
# spark.sql.iceberg.use-native-io=true # Cài đặt này có thể không cần thiết hoặc không được dùng nữa
spark.sql.parquet.compression.codec=gzip
spark.sql.iceberg.write.metadata.compression-codec=gzip
# spark.sql.iceberg.write.metadata.metrics.default=none # Cân nhắc bật lại nếu cần số liệu chi tiết

# Đặt datalake làm catalog mặc định
spark.sql.defaultCatalog=datalake

# Cấu hình Hadoop Catalog - Không sử dụng Nessie [13]
spark.sql.catalog.datalake=org.apache.iceberg.spark.SparkCatalog # Chỉ định lớp SparkCatalog
spark.sql.catalog.datalake.type=hadoop # Loại catalog là hadoop

# Cấu hình AWS S3/MinIO - Cập nhật để tương thích tốt hơn với MinIO [11, 14, 15]
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=admin # An toàn hơn khi sử dụng biến môi trường/secrets
spark.hadoop.fs.s3a.secret.key=admin123 # An toàn hơn khi sử dụng biến môi trường/secrets
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.connection.ssl.enabled=false # Đặt thành true nếu MinIO sử dụng TLS
spark.hadoop.fs.s3a.endpoint.region=us-east-1 # Đặt vùng nếu cần [14, 16]
# spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider # Không cần thiết nếu khóa được cung cấp trực tiếp

# Buộc sử dụng S3A cho tất cả các URL s3:, s3n:
spark.hadoop.fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3n.impl=org.apache.hadoop.fs.s3a.S3AFileSystem

# --- LOẠI BỎ Cấu hình S3A Committer - Không cần thiết cho Iceberg --- [4, 17]
# spark.hadoop.fs.s3a.committer.name=directory
# spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a=org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory

# --- LOẠI BỎ/Bình luận Cài đặt ổn định kết nối mở rộng (có thể thêm lại nếu cần) ---
spark.hadoop.fs.s3a.connection.timeout=1200000
spark.hadoop.fs.s3a.connection.maximum=100
spark.hadoop.fs.s3a.attempts.maximum=20
spark.hadoop.fs.s3a.retry.limit=20
spark.hadoop.fs.s3a.retry.interval=1000
spark.hadoop.fs.s3a.multipart.size=104857600
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.buffer=disk
spark.hadoop.fs.s3a.block.size=33554432

# Cài đặt tương thích quan trọng cho các cửa hàng tương thích S3 như MinIO
spark.hadoop.fs.s3a.change.detection.mode=none
spark.hadoop.fs.s3a.etag.checksum.enabled=false