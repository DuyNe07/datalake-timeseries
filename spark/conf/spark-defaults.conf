# Spark Configuration
spark.driver.memory=16g
spark.executor.memory=16g
spark.executor.cores=16
spark.driver.memoryOverhead=8g
spark.executor.memoryOverhead=8g
spark.sql.adaptive.enabled=true
spark.sql.files.maxPartitionBytes=134217728
spark.sql.shuffle.partitions=200
spark.sparkContext.setLogLevel="DEBUG" # Bỏ comment nếu cần ghi log chi tiết

# --- Sử dụng spark.jars.packages để quản lý phụ thuộc ---
# Đã loại bỏ dòng này vì các JAR đã được tải xuống trong Dockerfile

# Kích hoạt tiện ích mở rộng Iceberg và Nessie
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions

# Cấu hình ICEBERG
spark.sql.iceberg.handle-timestamp-without-timezone=true

# Đặt Nessie làm catalog mặc định
spark.sql.defaultCatalog=datalake

# Cấu hình Nessie Catalog (Sử dụng giao diện REST hiện đại)
spark.sql.catalog.datalake=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.datalake.type=rest
spark.sql.catalog.datalake.uri=http://nessie:19120/iceberg
# spark.sql.catalog.datalake.warehouse=s3a://datalake/ # Warehouse sẽ được Nessie quản lý
spark.sql.catalog.datalake.io-impl=org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.datalake.s3.endpoint=http://minio:9000
spark.sql.catalog.datalake.s3.path-style-access=true
spark.sql.catalog.datalake.s3.access-key-id=admin
spark.sql.catalog.datalake.s3.secret-access-key=admin123

# Cấu hình AWS/MinIO (Cấp Hadoop S3A)
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.access.key=admin
spark.hadoop.fs.s3a.secret.key=admin123
spark.hadoop.fs.s3a.connection.ssl.enabled=false
spark.hadoop.fs.s3a.endpoint.region=us-east-1
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider