{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1871ff5-d7c0-4fe8-9bbc-ed352f15ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame # type: ignore\n",
    "from pyspark.sql.functions import col, input_file_name, year, month, to_timestamp # type: ignore\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(\"InsertRawToBronze\")\n",
    "\n",
    "# Constants\n",
    "BASE_RAW_DATA_DIR = \"/src/data/raw/gold_test.csv\"\n",
    "TARGET_CATALOG = \"datalake\"\n",
    "TARGET_NAMESPACE = f\"{TARGET_CATALOG}.bronze\"\n",
    "DATE_FORMATS = [\n",
    "    \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\", \n",
    "    \"yyyy-MM-dd'T'HH:mm:ssZ\", \n",
    "    'yyyy-MM-dd HH:mm:ss.SSS', \n",
    "    'yyyy-MM-dd HH:mm:ss', \n",
    "    'MM/dd/yyyy HH:mm:ss', \n",
    "    'MM/dd/yyyy', \n",
    "    'yyyy-MM-dd', \n",
    "    'dd/MM/yyyy', \n",
    "    'dd-MM-yyyy', \n",
    "    'MM-dd-yyyy',\n",
    "    'MMM d, yyyy',\n",
    "    'MMMM d, yyyy'\n",
    "]\n",
    "\n",
    "# Standard column mapping\n",
    "STANDARD_COLUMNS = {\n",
    "    'date': ['date', 'datetime', 'time'],\n",
    "    'price': ['price', 'close'],\n",
    "    'open': ['open', 'Open'],\n",
    "    'high': ['high', 'High'],\n",
    "    'low': ['low'],\n",
    "    'volume': ['vol', 'vol.', 'volume', 'Volume'],\n",
    "    'change': ['change', 'change %'],\n",
    "    'id': ['id'],\n",
    "    'adj': ['adj', 'Adj'],\n",
    "    'price_tip': ['close_tip'],  \n",
    "    'adj_price': ['adj_close', 'adj close']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "976ac2e6-6c66-4a0e-b803-b22c3223ee59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "        SparkSession.builder.appName(\"Test_read\")\n",
    "        .enableHiveSupport()\n",
    "        .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "        .config(\"spark.sql.avro.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {SOURCE_NAMESPACE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ae088fd-1e0c-4bb6-aa37-b0d81c0ae527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables loaded successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    silver_indices = spark.table(\"datalake.gold.indices\")\n",
    "    silver_macro = spark.table(\"datalake.gold.macro\")\n",
    "    silver_usbonds = spark.table(\"datalake.gold.USbonds\")\n",
    "    \n",
    "    print(\"Tables loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading silver tables: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d1423c-bcfe-4f47-bb37-f248b81bd730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Clean all column names in DataFrame\"\"\"\n",
    "    original_columns = df.columns\n",
    "    new_columns = [clean_name(col) for col in original_columns]\n",
    "    \n",
    "    # Handle duplicate column names after cleaning\n",
    "    final_columns = []\n",
    "    seen = {}\n",
    "    for col in new_columns:\n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            final_columns.append(f\"{col}_{seen[col]}\")\n",
    "        else:\n",
    "            seen[col] = 0\n",
    "            final_columns.append(col)\n",
    "    \n",
    "    if original_columns != final_columns:\n",
    "        logger.warning(f\"Duplicate or invalid column names detected after cleaning. Renaming columns: {list(zip(original_columns, final_columns))}\")\n",
    "        return df.toDF(*final_columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74c0b6ed-27d9-4c2c-a177-cd1a1962d498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các cột có giá trị null: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|column_name|null_count|\n",
      "+-----------+----------+\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def standardize_schema(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Map columns to standard names and ensure consistent schema\"\"\"\n",
    "    # Log original columns for debugging\n",
    "    logger.info(f\"Original columns: {df.columns}\")\n",
    "    \n",
    "    # Create a mapping from actual column names to standard column names\n",
    "    column_mapping = {}\n",
    "    \n",
    "    # First, find all potential matches\n",
    "    potential_matches = {}\n",
    "    for std_col, possible_names in STANDARD_COLUMNS.items():\n",
    "        for actual_col in df.columns:\n",
    "            if actual_col.lower() in [name.lower() for name in possible_names]:\n",
    "                if std_col not in potential_matches:\n",
    "                    potential_matches[std_col] = []\n",
    "                potential_matches[std_col].append(actual_col)\n",
    "    \n",
    "    # Resolve the mappings, ensuring no duplicate target columns\n",
    "    used_columns = set()\n",
    "    for std_col, matched_columns in potential_matches.items():\n",
    "        if len(matched_columns) == 1:\n",
    "            # Only one match for this standard column\n",
    "            column_mapping[matched_columns[0]] = std_col\n",
    "            used_columns.add(matched_columns[0])\n",
    "        else:\n",
    "            # Multiple matches - need to create unique names\n",
    "            for idx, matched_col in enumerate(matched_columns):\n",
    "                if idx == 0:\n",
    "                    # First match gets the standard name\n",
    "                    column_mapping[matched_col] = std_col\n",
    "                else:\n",
    "                    # Subsequent matches get suffixed names\n",
    "                    column_mapping[matched_col] = f\"{std_col}_{idx}\"\n",
    "                used_columns.add(matched_col)\n",
    "    \n",
    "    # Include any remaining columns with their original names\n",
    "    for col in df.columns:\n",
    "        if col not in used_columns and col != \"_source_file\":\n",
    "            column_mapping[col] = col\n",
    "            \n",
    "    logger.info(f\"Column mapping: {column_mapping}\")\n",
    "    \n",
    "    # Create a standardized DataFrame\n",
    "    std_df = df\n",
    "    \n",
    "    # Rename columns according to mapping\n",
    "    for orig_col, std_col in column_mapping.items():\n",
    "        std_df = std_df.withColumnRenamed(orig_col, std_col)\n",
    "    \n",
    "    # Ensure source_file column exists\n",
    "    if \"_source_file\" in std_df.columns:\n",
    "        std_df = std_df.withColumnRenamed(\"_source_file\", \"source_file\")\n",
    "    \n",
    "    logger.info(f\"Standardized columns: {std_df.columns}\")\n",
    "    return std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba87c155-8167-4867-b77c-97dbe16a60cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các cột có giá trị null: ['gold', 'oil', 'us_dollar', 'usd_vnd']\n",
      "+-----------+----------+\n",
      "|column_name|null_count|\n",
      "+-----------+----------+\n",
      "|       gold|         2|\n",
      "|        oil|         2|\n",
      "|  us_dollar|         2|\n",
      "|    usd_vnd|         1|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"mode\", \"PERMISSIVE\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .csv(BASE_RAW_DATA_DIR)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e23ad30d-db90-476b-bde0-285e66e3f2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các cột có giá trị null: ['gold', 'us_2_year_bond', 'us_5_year_bond']\n",
      "+--------------+----------+\n",
      "|   column_name|null_count|\n",
      "+--------------+----------+\n",
      "|          gold|         1|\n",
      "|us_2_year_bond|         1|\n",
      "|us_5_year_bond|         1|\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_date_column(df: DataFrame, date_col_name: str) -> DataFrame:\n",
    "    \"\"\"Try to parse the date column using multiple formats\"\"\"\n",
    "    logger.info(f\"Attempting to parse date column '{date_col_name}' using formats: {DATE_FORMATS}\")\n",
    "    \n",
    "    # Create a new DataFrame with a parsed date column\n",
    "    parsed_df = df\n",
    "    \n",
    "    # Try each date format in succession\n",
    "    for idx, date_format in enumerate(DATE_FORMATS):\n",
    "        try:\n",
    "            logger.info(f\"Trying date format #{idx+1}: {date_format}\")\n",
    "            # Create temporary column with this format\n",
    "            temp_df = parsed_df.withColumn(\n",
    "                \"parsed_date\", \n",
    "                to_timestamp(col(date_col_name), date_format)\n",
    "            )\n",
    "            \n",
    "            # Count non-null values with this format\n",
    "            valid_count = temp_df.filter(col(\"parsed_date\").isNotNull()).count()\n",
    "            logger.info(f\"Format {date_format} produced {valid_count} valid dates\")\n",
    "            \n",
    "            if valid_count > 0:\n",
    "                # Use this format since it worked for some values\n",
    "                parsed_df = temp_df\n",
    "                logger.info(f\"Using format '{date_format}' for date parsing\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Format '{date_format}' failed with error: {str(e)}\")\n",
    "    \n",
    "    return parsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "820cae27-e8e9-4b9c-b6fe-2bb3c0338496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_partition_columns(df: DataFrame) -> Tuple[DataFrame, List[str]]:\n",
    "    \"\"\"Add year and month partition columns from date column\"\"\"\n",
    "    logger.info(\"Attempting to add partition columns...\")\n",
    "    \n",
    "    # Find date column (case insensitive)\n",
    "    date_col_candidates = [c for c in df.columns if c.lower() == \"date\"]\n",
    "    \n",
    "    if not date_col_candidates:\n",
    "        logger.warning(\"No 'date' column found. Skipping partitioning.\")\n",
    "        return df, []\n",
    "    \n",
    "    date_col_name = date_col_candidates[0]\n",
    "    logger.info(f\"Found date column: '{date_col_name}'\")\n",
    "    \n",
    "    try:\n",
    "        # Parse the date column\n",
    "        df_with_parsed_date = parse_date_column(df, date_col_name)\n",
    "        \n",
    "        # Check if we successfully parsed any dates\n",
    "        if \"parsed_date\" not in df_with_parsed_date.columns:\n",
    "            logger.warning(\"Failed to parse date column with any format. Skipping partitioning.\")\n",
    "            return df, []\n",
    "            \n",
    "        # Add year and month columns from the parsed date\n",
    "        df_partitioned = (\n",
    "            df_with_parsed_date\n",
    "            .withColumn(\"year\", year(col(\"parsed_date\")))\n",
    "            .withColumn(\"month\", month(col(\"parsed_date\")))\n",
    "        )\n",
    "        \n",
    "        # Check if partition columns were successfully added\n",
    "        null_years = df_partitioned.filter(col(\"year\").isNull()).count()\n",
    "        if null_years > 0:\n",
    "            logger.warning(f\"{null_years} rows have NULL year values after date parsing\")\n",
    "        \n",
    "        logger.info(f\"Successfully added partition columns 'year', 'month' from '{date_col_name}'\")\n",
    "        \n",
    "        # Return the DataFrame with the partition columns and without the temporary parsed_date column\n",
    "        return df_partitioned.drop(\"parsed_date\"), [\"year\", \"month\"]\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error adding partition columns: {str(e)}\")\n",
    "        logger.error(\"Continuing without partition columns\")\n",
    "        return df, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e7982-1783-4afc-a144-8a21c9b0191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_iceberg(df: DataFrame, table_name: str, partition_by: List[str]):\n",
    "    \"\"\"Write DataFrame to Iceberg table\"\"\"\n",
    "    logger.info(f\"Starting write operation to Iceberg table: {table_name}\")\n",
    "    logger.info(f\"Partitioning by: {partition_by}\")\n",
    "    logger.info(f\"Write mode: overwrite\")\n",
    "    logger.info(f\"DataFrame contains {df.count()} rows to write\")\n",
    "    \n",
    "    # Check for duplicate column names before writing\n",
    "    df = check_duplicate_columns(df)\n",
    "    \n",
    "    try:\n",
    "        writer = (\n",
    "            df.write\n",
    "            .format(\"iceberg\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "        )\n",
    "        \n",
    "        if partition_by and all(p in df.columns for p in partition_by):\n",
    "            # Ensure partition columns have valid data\n",
    "            null_partition_count = df.filter(\n",
    "                ' OR '.join([f\"{p} IS NULL\" for p in partition_by])\n",
    "            ).count()\n",
    "            \n",
    "            if null_partition_count > 0:\n",
    "                logger.warning(f\"{null_partition_count} rows have NULL partition values\")\n",
    "            \n",
    "            if null_partition_count < df.count():  # Only partition if some rows have valid values\n",
    "                writer = writer.partitionBy(*partition_by)\n",
    "                logger.info(f\"Partitioning by {partition_by}\")\n",
    "            else:\n",
    "                logger.warning(\"Skipping partitioning as all partition columns contain NULL values\")\n",
    "        else:\n",
    "            logger.warning(f\"Partition columns missing from DataFrame. Skipping partitioning.\")\n",
    "        \n",
    "        writer.saveAsTable(table_name)\n",
    "        logger.info(f\"Successfully wrote data to Iceberg table: {table_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write to Iceberg table {table_name}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09b1652-8534-4631-9c19-a1704512a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42d805-be78-4447-8293-abb046ca9409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
