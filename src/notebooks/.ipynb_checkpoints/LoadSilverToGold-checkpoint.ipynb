{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data from Silver to Gold Layer\n",
    "\n",
    "This notebook reads data from silver tables (gold, oil, us_dollar, usd_vnd), combines them based on date, and creates a gold table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, last, row_number, to_timestamp, year, avg, monotonically_increasing_id, lit\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Silver_To_Gold\").getOrCreate()\n",
    "\n",
    "# Create gold namespace if not exists\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS datalake.gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Silver Tables\n",
    "\n",
    "Read the required silver tables: gold, oil, us_dollar, usd_vnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Read silver tables\n",
    "try:\n",
    "    silver_gold = spark.table(\"datalake.silver.gold\")\n",
    "    silver_oil = spark.table(\"datalake.silver.oil\")\n",
    "    silver_us_dollar = spark.table(\"datalake.silver.us_dollar\")\n",
    "    silver_usd_vnd = spark.table(\"datalake.silver.usd_vnd\")\n",
    "    \n",
    "    print(\"Tables loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading silver tables: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oil schema:\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-----+-----+\n",
      "| ID|      Date|Price| Open| High|  Low|\n",
      "+---+----------+-----+-----+-----+-----+\n",
      "|  1|1995-01-03|17.44|17.65|17.82|17.41|\n",
      "|  2|1995-01-04|17.48|17.42|17.65|17.35|\n",
      "|  3|1995-01-05|17.72|17.48|17.77|17.45|\n",
      "|  4|1995-01-06|17.67|17.72|18.02|17.62|\n",
      "|  5|1995-01-09| 17.4|17.67|17.68|17.39|\n",
      "+---+----------+-----+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "US Dollar schema:\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      "\n",
      "+---+----------+-----+-----+-----+-----+\n",
      "| ID|      Date|Price| Open| High|  Low|\n",
      "+---+----------+-----+-----+-----+-----+\n",
      "|  1|1995-01-03|89.21|89.27|89.27|88.96|\n",
      "|  2|1995-01-04|89.35|89.25|89.53|89.19|\n",
      "|  3|1995-01-05|89.04|89.33| 89.2|88.93|\n",
      "|  4|1995-01-06|89.71|89.01|89.79|88.96|\n",
      "|  5|1995-01-09|88.43|89.59|89.64|88.25|\n",
      "+---+----------+-----+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "USD-VND schema:\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      "\n",
      "+---+----------+-------+-------+-------+-------+\n",
      "| ID|      Date|  Price|   Open|   High|    Low|\n",
      "+---+----------+-------+-------+-------+-------+\n",
      "|  1|1995-01-02|11042.0|11042.0|11042.0|11042.0|\n",
      "|  2|1995-01-03|11042.0|11042.0|11042.0|11042.0|\n",
      "|  3|1995-01-04|11040.0|11040.0|11040.0|11040.0|\n",
      "|  4|1995-01-05|11040.0|11040.0|11040.0|11040.0|\n",
      "|  5|1995-01-06|11035.0|11035.0|11035.0|11035.0|\n",
      "+---+----------+-------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display table schemas and sample data\n",
    "print(\"Gold schema:\")\n",
    "silver_gold.printSchema()\n",
    "silver_gold.show(5)\n",
    "\n",
    "print(\"Oil schema:\")\n",
    "silver_oil.printSchema()\n",
    "silver_oil.show(5)\n",
    "\n",
    "print(\"US Dollar schema:\")\n",
    "silver_us_dollar.printSchema()\n",
    "silver_us_dollar.show(5)\n",
    "\n",
    "print(\"USD-VND schema:\")\n",
    "silver_usd_vnd.printSchema()\n",
    "silver_usd_vnd.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# silver_gold.filter(col(\"Price\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_oil.filter(col(\"Price\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_us_dollar.filter(col(\"Price\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_usd_vnd.filter(col(\"Price\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Prepare Data\n",
    "\n",
    "Extract Date and Price columns from each table and rename them appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Oil dataframe:\n",
      "+----------+-----+\n",
      "|      Date|  oil|\n",
      "+----------+-----+\n",
      "|1995-01-03|17.44|\n",
      "|1995-01-04|17.48|\n",
      "|1995-01-05|17.72|\n",
      "|1995-01-06|17.67|\n",
      "|1995-01-09| 17.4|\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Prepared US Dollar dataframe:\n",
      "+----------+---------+\n",
      "|      Date|us_dollar|\n",
      "+----------+---------+\n",
      "|1995-01-03|    89.21|\n",
      "|1995-01-04|    89.35|\n",
      "|1995-01-05|    89.04|\n",
      "|1995-01-06|    89.71|\n",
      "|1995-01-09|    88.43|\n",
      "+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Prepared USD-VND dataframe:\n",
      "+----------+-------+\n",
      "|      Date|usd_vnd|\n",
      "+----------+-------+\n",
      "|1995-01-02|11042.0|\n",
      "|1995-01-03|11042.0|\n",
      "|1995-01-04|11040.0|\n",
      "|1995-01-05|11040.0|\n",
      "|1995-01-06|11035.0|\n",
      "+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gold_df = silver_gold.select(col(\"Date\"), col(\"Price\").alias(\"gold\"))\n",
    "oil_df = silver_oil.select(col(\"Date\"), col(\"Price\").alias(\"oil\"))\n",
    "us_dollar_df = silver_us_dollar.select(col(\"Date\"), col(\"Price\").alias(\"us_dollar\"))\n",
    "usd_vnd_df = silver_usd_vnd.select(col(\"Date\"), col(\"Price\").alias(\"usd_vnd\"))\n",
    "\n",
    "print(\"Prepared Gold dataframe:\")\n",
    "gold_df.show(5)\n",
    "\n",
    "print(\"Prepared Oil dataframe:\")\n",
    "oil_df.show(5)\n",
    "\n",
    "print(\"Prepared US Dollar dataframe:\")\n",
    "us_dollar_df.show(5)\n",
    "\n",
    "print(\"Prepared USD-VND dataframe:\")\n",
    "usd_vnd_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Table With Most Data\n",
    "\n",
    "Count records in each table to find which one has the most data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oil records: 7689\n",
      "US Dollar records: 7637\n",
      "USD-VND records: 7657\n"
     ]
    }
   ],
   "source": [
    "# Count records in each table\n",
    "gold_count = gold_df.count()\n",
    "oil_count = oil_df.count()\n",
    "us_dollar_count = us_dollar_df.count()\n",
    "usd_vnd_count = usd_vnd_df.count()\n",
    "\n",
    "print(f\"Gold records: {gold_count}\")\n",
    "print(f\"Oil records: {oil_count}\")\n",
    "print(f\"US Dollar records: {us_dollar_count}\")\n",
    "print(f\"USD-VND records: {usd_vnd_count}\")\n",
    "\n",
    "# # Determine which table has the most records\n",
    "# counts = {\n",
    "#     \"gold\": gold_count,\n",
    "#     \"oil\": oil_count,\n",
    "#     \"us_dollar\": us_dollar_count,\n",
    "#     \"usd_vnd\": usd_vnd_count\n",
    "# }\n",
    "\n",
    "# base_table_name = max(counts, key=counts.get)\n",
    "# print(f\"Base table with most records: {base_table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the base dataframe based on which has the most records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      Date| gold|\n",
      "+----------+-----+\n",
      "|1995-01-03|380.9|\n",
      "|1995-01-04|375.3|\n",
      "|1995-01-05|376.6|\n",
      "|1995-01-06|372.2|\n",
      "|1995-01-09|374.0|\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_df = gold_df\n",
    "base_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Tables by Date\n",
    "\n",
    "Join all tables by Date, using left outer joins to maintain all dates from the base table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dfs = [gold_df, oil_df, us_dollar_df, usd_vnd_df]\n",
    "\n",
    "# Outer join all DataFrames on 'Date'\n",
    "final_df = reduce(lambda df1, df2: df1.join(df2, on='Date', how='outer'), dfs)\n",
    "\n",
    "# Optional: sort by date\n",
    "final_df = final_df.orderBy('Date')\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- oil: double (nullable = true)\n",
      " |-- us_dollar: double (nullable = true)\n",
      " |-- usd_vnd: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs = [gold_df, oil_df, us_dollar_df, usd_vnd_df]\n",
    "final_df = reduce(lambda df1, df2: df1.join(df2, on='Date', how='outer'), dfs)\n",
    "final_df = final_df.orderBy('Date') \n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_1 = gold_df.join(oil_df, on=\"Date\", how=\"left\")\n",
    "merged_df_2 = merged_df_1.join(us_dollar_df, on=\"Date\", how=\"left\")\n",
    "merged_df_3 = merged_df_2.join(usd_vnd_df, on=\"Date\", how=\"left\")\n",
    "final_df = merged_df_3.withColumn(\"ID\", monotonically_increasing_id()) \\\n",
    "                    .select(\"ID\", \"Date\", \"gold\", \"oil\", \"us_dollar\", \"usd_vnd\") \\\n",
    "                    .orderBy(\"Date\")\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Gold Layer\n",
    "\n",
    "Save the combined table to the gold layer as 'gold_oil_usd_vnd'\n",
    "\n",
    "Write to gold layer using Iceberg format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# A fatal error has been detected by the Java Runtime Environment:\n",
      "#\n",
      "#  SIGSEGV (0xb) at pc=0x00007f58d85d4a68, pid=2761, tid=2841\n",
      "#\n",
      "# JRE version: OpenJDK Runtime Environment (17.0.14+7) (build 17.0.14+7-Debian-1deb11u1)\n",
      "# Java VM: OpenJDK 64-Bit Server VM (17.0.14+7-Debian-1deb11u1, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64)\n",
      "# Problematic frame:\n",
      "# V  [libjvm.so+0xe2ca68]  SymbolTable::do_lookup(char const*, int, unsigned long)+0xd8\n",
      "#\n",
      "# Core dump will be written. Default location: Core dumps may be processed with \"/usr/share/apport/apport -p%p -s%s -c%c -d%d -P%P -u%u -g%g -- %E\" (or dumping to /src/notebooks/core.2761)\n",
      "#\n",
      "# An error report file with more information is saved as:\n",
      "# /src/notebooks/hs_err_pid2761.log\n",
      "#\n",
      "# If you would like to submit a bug report, please visit:\n",
      "#   https://bugs.debian.org/openjdk-17\n",
      "#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error writing to gold table: An error occurred while calling o115.saveAsTable\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    final_df.write \\\n",
    "        .format(\"iceberg\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"datalake.gold.gold_oil_usd_vnd\")\n",
    "    \n",
    "    print(\"Successfully wrote combined data to datalake.gold.gold_oil_usd_vnd\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to gold table: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the gold table was created successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    gold_table = spark.table(\"datalake.gold.gold_oil_usd_vnd\")\n",
    "    print(\"Gold table schema:\")\n",
    "    gold_table.printSchema()\n",
    "    \n",
    "    print(\"Gold table sample data:\")\n",
    "    gold_table.show(10)\n",
    "    \n",
    "    print(f\"Total records in gold table: {gold_table.count()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading gold table: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Perform some basic analysis on the combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in each column\n",
    "from pyspark.sql.functions import count, when, col, isnan\n",
    "\n",
    "null_counts = gold_table.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in gold_table.columns])\n",
    "print(\"Null value counts by column:\")\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for each price column\n",
    "print(\"Summary statistics:\")\n",
    "gold_table.select(\"gold\", \"oil\", \"us_dollar\", \"usd_vnd\").summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add year column for yearly analysis\n",
    "gold_table_with_year = gold_table.withColumn(\"Year\", year(\"Date\"))\n",
    "\n",
    "# Calculate yearly averages\n",
    "yearly_avgs = gold_table_with_year.groupBy(\"Year\").agg(\n",
    "    avg(\"gold\").alias(\"gold_avg\"),\n",
    "    avg(\"oil\").alias(\"oil_avg\"),\n",
    "    avg(\"us_dollar\").alias(\"us_dollar_avg\"),\n",
    "    avg(\"usd_vnd\").alias(\"usd_vnd_avg\")\n",
    ")\n",
    "\n",
    "print(\"Yearly averages:\")\n",
    "yearly_avgs.orderBy(\"Year\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Stop Spark session when done\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark session stopped.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1796\u001b[0m, in \u001b[0;36mSparkSession.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;124;03mStop the underlying :class:`SparkContext`.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;124;03m>>> spark.stop()  # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SQLContext\n\u001b[0;32m-> 1796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[38;5;66;03m# We should clean the default session up. See SPARK-23228.\u001b[39;00m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/context.py:654\u001b[0m, in \u001b[0;36mSparkContext.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_jsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JError:\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;66;03m# Case: SPARK-18523\u001b[39;00m\n\u001b[1;32m    657\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to cleanly shutdown Spark JVM process.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m It is possible that the process has crashed,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m been killed or may also be in a zombie state.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    661\u001b[0m             \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[1;32m    662\u001b[0m         )\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark session when done\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
