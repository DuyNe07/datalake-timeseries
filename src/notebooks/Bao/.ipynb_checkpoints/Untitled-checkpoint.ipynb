{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2cd57c-a4ea-4b67-9b3a-0c69edced5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, Window # type: ignore\n",
    "from pyspark.sql.functions import (col, to_timestamp, year, month, regexp_replace, when, # type: ignore\n",
    "                                  date_format, to_date, sequence, explode, lit, min as min_,\n",
    "                                  max as max_, count, last, row_number, concat, current_timestamp, rank) # type: ignore\n",
    "import pyspark.sql.functions as F # type: ignore\n",
    "\n",
    "from pyspark.sql.window import Window # type: ignore\n",
    "from pyspark.sql.types import StringType, DoubleType, TimestampType, DateType # type: ignore\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(\"InsertBronzeToSilver\")\n",
    "\n",
    "# Constants\n",
    "SOURCE_CATALOG = \"datalake\"\n",
    "SOURCE_NAMESPACE = f\"{SOURCE_CATALOG}.bronze\"\n",
    "TARGET_CATALOG = \"datalake\"\n",
    "TARGET_NAMESPACE = f\"{TARGET_CATALOG}.silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "828fec0e-ada3-42d8-91a5-46b2c25dfc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINANCIAL_COLUMNS = [\"price\", \"open\", \"high\", \"low\", \"volume\"]\n",
    "START_DATE = \"1995-01-05\"  # Start date for continuous date generation\n",
    "END_DATE = \"2025-03-07\"    # End date for continuous date generation\n",
    "# Date column\n",
    "DATE_COLUMN = \"date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "559f11d4-eab1-47a2-838d-8ed81a76267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session(app_name: str) -> SparkSession:\n",
    "    \"\"\"Initialize Spark session with required configurations\"\"\"\n",
    "    logger.info(\"Initializing Spark Session...\")\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(app_name)\n",
    "        .enableHiveSupport()\n",
    "        .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "        .config(\"spark.sql.avro.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    \n",
    "    spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "\n",
    "    \n",
    "    logger.info(f\"Spark Session initialized successfully. Spark version: {spark.version}\")\n",
    "    \n",
    "    # Log important configs\n",
    "    conf = spark.sparkContext.getConf()\n",
    "    s3_endpoint = conf.get(\"spark.hadoop.fs.s3a.endpoint\", \"N/A\")\n",
    "    catalog_type = conf.get(f\"spark.sql.catalog.{TARGET_CATALOG}.type\", \"N/A\")\n",
    "    catalog_uri = conf.get(f\"spark.sql.catalog.{TARGET_CATALOG}.uri\", \"N/A\")\n",
    "    warehouse = conf.get(f\"spark.sql.catalog.{TARGET_CATALOG}.warehouse\", \"N/A\")\n",
    "    \n",
    "    logger.info(f\"Source Namespace: {SOURCE_NAMESPACE}\")\n",
    "    logger.info(f\"Target Namespace: {TARGET_NAMESPACE}\")\n",
    "    logger.info(f\"S3 Endpoint (Hadoop): {s3_endpoint}\")\n",
    "    logger.info(f\"Catalog Type: {catalog_type}\")\n",
    "    logger.info(f\"Catalog URI: {catalog_uri}\")\n",
    "    logger.info(f\"Catalog Warehouse: {warehouse}\")\n",
    "    \n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ce44c5-c8d9-4a3e-ad80-a01567a7f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_namespace_if_not_exists(spark: SparkSession, namespace: str):\n",
    "    \"\"\"Create Iceberg namespace if it doesn't exist\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Checking/Creating namespace: {namespace}\")\n",
    "        spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {namespace}\")\n",
    "        logger.info(f\"Namespace {namespace} ensured.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create namespace {namespace}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd4c4625-215d-44ce-ada7-6497d2a1447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_tables(spark: SparkSession, namespace: str) -> List[str]:\n",
    "    \"\"\"List all tables in a namespace\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Listing tables in namespace: {namespace}\")\n",
    "        tables_df = spark.sql(f\"SHOW TABLES IN {namespace}\")\n",
    "        \n",
    "        if tables_df.count() == 0:\n",
    "            logger.warning(f\"No tables found in namespace {namespace}\")\n",
    "            return []\n",
    "            \n",
    "        table_names = [row.tableName for row in tables_df.select(\"tableName\").collect()]\n",
    "        logger.info(f\"Found {len(table_names)} tables: {', '.join(table_names)}\")\n",
    "        return table_names\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to list tables in {namespace}: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a9b1bd-8f34-47bc-b361-e99ec4ee53e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_based_forward_fill(df, date_col=\"date\", partition_cols=None):\n",
    "    \"\"\"\n",
    "    Perform forward fill on all columns except date_col and partition_cols.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input PySpark DataFrame (e.g., after left join with date range).\n",
    "    - date_col: Name of the date column.\n",
    "    - partition_cols: List of column names to partition by (e.g., [\"symbol\"]). Use None for no partitioning.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with forward-filled missing values.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"year\", year(col(DATE_COLUMN))).withColumn(\"month\", month(col(DATE_COLUMN)))\n",
    "    \n",
    "    partition_cols = ['year', 'month']\n",
    "\n",
    "    # Define window: partition by optional keys and order by date ascending\n",
    "    window_spec = Window.partitionBy(*partition_cols).orderBy(DATE_COLUMN).rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "    # Forward fill for all columns except partition + date\n",
    "    columns_to_fill = [c for c in df.columns if c not in [DATE_COLUMN, 'year', 'month', 'source_file', 'inserted']]\n",
    "\n",
    "    for c in columns_to_fill:\n",
    "        df = df.withColumn(c, last(c, ignorenulls=True).over(window_spec))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c318b45-098f-493b-a5e1-0bd514b63986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_range_df(spark: SparkSession) -> DataFrame:\n",
    "    \"\"\"Create a DataFrame with continuous date range\"\"\"\n",
    "    logger.info(f\"Creating date range from {START_DATE} to {END_DATE}...\")\n",
    "    \n",
    "    date_range_df = spark.sql(f\"\"\"\n",
    "        SELECT explode(sequence(\n",
    "            to_date('{START_DATE}'), \n",
    "            to_date('{END_DATE}'), \n",
    "            interval 1 day\n",
    "        )) as {DATE_COLUMN}\n",
    "    \"\"\")\n",
    "    \n",
    "    logger.info(f\"Created continuous date range with {date_range_df.count()} dates\")\n",
    "    return date_range_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d66779-1c24-4f25-91ee-cbc73ac148a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_forward_fill(spark: SparkSession, df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"Process DataFrame with continuous dates and forward fill\"\"\"\n",
    "    logger.info(f\"Processing {table_name} with forward fill...\")\n",
    "    \n",
    "    # 1. Get the full date range DataFrame\n",
    "    date_range_df = create_date_range_df(spark)\n",
    "    \n",
    "    # 2. Join with existing data\n",
    "    logger.info(\"Joining with full date range...\")\n",
    "    full_df = date_range_df.join(df, on=DATE_COLUMN, how=\"left\")\n",
    "    \n",
    "    # 3. Order by date for forward fill\n",
    "    full_df = full_df.orderBy(DATE_COLUMN)\n",
    "    \n",
    "    # 4. Apply segment-based forward fill for all tables\n",
    "    result_df = segment_based_forward_fill(full_df, table_name)\n",
    "    \n",
    "    # 7. Show sample data\n",
    "    logger.info(\"Sample data after processing:\")\n",
    "    result_df.orderBy(DATE_COLUMN).limit(3).show(truncate=False)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e57e5a25-9d53-40be-8d1e-29eca4a4a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_cast_columns(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Clean and cast columns to appropriate types\"\"\"\n",
    "    logger.info(\"Cleaning and casting columns to appropriate types...\")\n",
    "    \n",
    "    # Ensure date column is date type\n",
    "    if DATE_COLUMN in df.columns:\n",
    "        date_type = str(df.schema[DATE_COLUMN].dataType)\n",
    "        logger.info(f\"Date column type: {date_type}\")\n",
    "        \n",
    "        if not (isinstance(df.schema[DATE_COLUMN].dataType, DateType) or \n",
    "                isinstance(df.schema[DATE_COLUMN].dataType, TimestampType)):\n",
    "            logger.info(\"Converting date column to date type\")\n",
    "            \n",
    "            # Try multiple date formats\n",
    "            df = df.withColumn(\n",
    "                DATE_COLUMN,\n",
    "                F.coalesce(\n",
    "                    to_timestamp(col(DATE_COLUMN), \"MM-dd-yyyy\")\n",
    "                ).cast(\"date\")\n",
    "            )\n",
    "    \n",
    "    # Process financial columns\n",
    "    for column in FINANCIAL_COLUMNS:\n",
    "        if column in df.columns:\n",
    "            df = df.withColumn(\n",
    "                column,\n",
    "                regexp_replace(\n",
    "                    regexp_replace(col(column).cast(\"string\"), \",\", \"\"),\n",
    "                    \"%\", \"\"\n",
    "                )\n",
    "            )\n",
    "            logger.info(f\"Cleaned string values in column '{column}'\")\n",
    "            \n",
    "            # Cast to float\n",
    "            df = df.withColumn(column, col(column).cast(\"double\"))\n",
    "            logger.info(f\"Converted column '{column}' to double type\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aac8acb5-128e-433d-95ad-151f7272604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(spark: SparkSession, df: DataFrame, table_name: str) -> DataFrame:\n",
    "    \"\"\"Main transformation function\"\"\"\n",
    "    logger.info(f\"Transforming data for table {table_name}\")\n",
    "    \n",
    "    # 1. Standardize column names\n",
    "    window_spec = Window.partitionBy(\"date\").orderBy(col(\"inserted\").desc())\n",
    "\n",
    "    # Add a rank column\n",
    "    df_ranked = df.withColumn(\"rnk\", rank().over(window_spec))\n",
    "    \n",
    "    # Filter to only the top-ranked (latest inserted) row per date\n",
    "    df = df_ranked.filter(col(\"rnk\") == 1).drop(\"rnk\")\n",
    "    \n",
    "    # 2. Clean and cast columns\n",
    "    df = clean_and_cast_columns(df)\n",
    "    \n",
    "    # 3. Filter to include only data from START_DATE onward\n",
    "    logger.info(f\"Filtering data from {START_DATE} onward\")\n",
    "    df = df.filter(col(DATE_COLUMN) >= START_DATE)\n",
    "    \n",
    "    # 4. Process with continuous date range and forward fill\n",
    "    df = process_with_forward_fill(spark, df, table_name)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db0447a8-2885-49c0-bb7c-110c1a195975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_iceberg(df: DataFrame, table_name: str, partition_by: List[str]):\n",
    "    \"\"\"Write DataFrame to Iceberg table\"\"\"\n",
    "    logger.info(f\"Writing to Iceberg table: {table_name}\")\n",
    "    logger.info(f\"Partitioning by: {partition_by}\")\n",
    "    \n",
    "    # Check for duplicate columns\n",
    "    df = df.drop(col('inserted')).withColumn(\"inserted\", current_timestamp())\n",
    "\n",
    "    try:\n",
    "        # Prepare writer\n",
    "        writer = (\n",
    "            df.write\n",
    "            .format(\"iceberg\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .partitionBy(*partition_by)\n",
    "            #.option(\"iceberg.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "        )        \n",
    "        # Write to table\n",
    "        writer.saveAsTable(table_name)\n",
    "        logger.info(f\"Successfully wrote to {table_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write to table {table_name}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4d922eb-0585-475e-a529-6542ba38ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table(spark: SparkSession, table_name: str):\n",
    "    \"\"\"Process a single table from bronze to silver\"\"\"\n",
    "    logger.info(f\"=== Processing table: {table_name} ===\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # 1. Read from bronze\n",
    "        source_table = f\"{SOURCE_NAMESPACE}.{table_name}\"\n",
    "        logger.info(f\"Reading from {source_table}\")\n",
    "        \n",
    "        df = spark.table(source_table)\n",
    "        if df.rdd.isEmpty():\n",
    "            logger.warning(f\"No data found in {source_table}. Skipping.\")\n",
    "            return\n",
    "        \n",
    "        # 2. Display schema\n",
    "        logger.info(\"Table schema:\")\n",
    "        df.printSchema()\n",
    "        \n",
    "        # 3. Transform data\n",
    "        df_transformed = transform_data(spark, df, table_name)\n",
    "        \n",
    "        # 4. Write to silver\n",
    "        target_table = f\"{TARGET_NAMESPACE}.{table_name}\"\n",
    "        write_to_iceberg(df_transformed, target_table, [\"year\", \"month\"])\n",
    "        \n",
    "        # 5. Log success\n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        logger.info(f\"=== Successfully processed {table_name} in {duration} ===\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {table_name}: {str(e)}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "891085a3-413d-4bb4-9a4b-066baf780fe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 17:47:32,598 - INFO - Initializing Spark Session...\n",
      "2025-05-24 17:47:33,492 - INFO - Spark Session initialized successfully. Spark version: 3.5.5\n",
      "2025-05-24 17:47:33,520 - INFO - Source Namespace: datalake.bronze\n",
      "2025-05-24 17:47:33,521 - INFO - Target Namespace: datalake.silver\n",
      "2025-05-24 17:47:33,522 - INFO - S3 Endpoint (Hadoop): http://minio:9000\n",
      "2025-05-24 17:47:33,522 - INFO - Catalog Type: rest\n",
      "2025-05-24 17:47:33,523 - INFO - Catalog URI: http://nessie:19120/iceberg\n",
      "2025-05-24 17:47:33,523 - INFO - Catalog Warehouse: N/A\n",
      "2025-05-24 17:47:33,524 - INFO - Checking/Creating namespace: datalake.silver\n",
      "2025-05-24 17:47:35,095 - INFO - Namespace datalake.silver ensured.\n",
      "2025-05-24 17:47:35,097 - INFO - Listing tables in namespace: datalake.bronze\n",
      "2025-05-24 17:47:38,054 - INFO - Found 19 tables: dow_jones, gold, inflation, interest, msci_world, nasdaq100, oil, russell2000, s_p500, us_10_year_bond, us_2_year_bond, us_3_month_bond, us_5_year_bond, us_dollar, usd_vnd, vn_10_year_bond, vn_2_year_bond, vn_5_year_bond, vnd_usd\n",
      "2025-05-24 17:47:38,056 - INFO - === Processing table: dow_jones ===\n",
      "2025-05-24 17:47:38,057 - INFO - Reading from datalake.bronze.dow_jones\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "2025-05-24 17:47:40,970 - INFO - Table schema:                                  \n",
      "2025-05-24 17:47:40,975 - INFO - Transforming data for table dow_jones\n",
      "2025-05-24 17:47:41,081 - INFO - Cleaning and casting columns to appropriate types...\n",
      "2025-05-24 17:47:41,084 - INFO - Date column type: StringType()\n",
      "2025-05-24 17:47:41,085 - INFO - Converting date column to date type\n",
      "2025-05-24 17:47:41,148 - INFO - Cleaned string values in column 'price'\n",
      "2025-05-24 17:47:41,165 - INFO - Converted column 'price' to double type\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- open: string (nullable = true)\n",
      " |-- high: string (nullable = true)\n",
      " |-- low: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- inserted: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 17:47:41,193 - INFO - Cleaned string values in column 'open'\n",
      "2025-05-24 17:47:41,209 - INFO - Converted column 'open' to double type\n",
      "2025-05-24 17:47:41,235 - INFO - Cleaned string values in column 'high'\n",
      "2025-05-24 17:47:41,252 - INFO - Converted column 'high' to double type\n",
      "2025-05-24 17:47:41,276 - INFO - Cleaned string values in column 'low'\n",
      "2025-05-24 17:47:41,292 - INFO - Converted column 'low' to double type\n",
      "2025-05-24 17:47:41,318 - INFO - Cleaned string values in column 'volume'\n",
      "2025-05-24 17:47:41,332 - INFO - Converted column 'volume' to double type\n",
      "2025-05-24 17:47:41,333 - INFO - Filtering data from 1995-01-05 onward\n",
      "2025-05-24 17:47:41,351 - INFO - Processing dow_jones with forward fill...\n",
      "2025-05-24 17:47:41,352 - INFO - Creating date range from 1995-01-05 to 2025-03-07...\n",
      "2025-05-24 17:47:41,656 - INFO - Created continuous date range with 11020 dates\n",
      "2025-05-24 17:47:41,658 - INFO - Joining with full date range...\n",
      "2025-05-24 17:47:41,911 - INFO - Sample data after processing:\n",
      "2025-05-24 17:47:46,354 - INFO - Writing to Iceberg table: datalake.silver.dow_jones\n",
      "2025-05-24 17:47:46,356 - INFO - Partitioning by: ['year', 'month']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+-------+-------+-------+---------------------------------------------+----+-----+--------------------------+\n",
      "|date      |open  |high   |low    |price  |volume |source_file                                  |year|month|inserted                  |\n",
      "+----------+------+-------+-------+-------+-------+---------------------------------------------+----+-----+--------------------------+\n",
      "|1995-01-05|3857.6|3860.68|3843.19|3850.92|2.581E7|file:///src/data/raw/Dow%20Jones/DowJones.csv|1995|1    |2025-05-24 10:14:39.409379|\n",
      "|1995-01-06|3850.9|3887.26|3841.84|3867.41|3.024E7|file:///src/data/raw/Dow%20Jones/DowJones.csv|1995|1    |2025-05-24 10:14:39.409379|\n",
      "|1995-01-07|3850.9|3887.26|3841.84|3867.41|3.024E7|NULL                                         |1995|1    |NULL                      |\n",
      "+----------+------+-------+-------+-------+-------+---------------------------------------------+----+-----+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 17:47:58,195 - INFO - Successfully wrote to datalake.silver.dow_jones\n",
      "2025-05-24 17:47:58,197 - INFO - === Successfully processed dow_jones in 0:00:20.140675 ===\n",
      "2025-05-24 17:47:58,199 - INFO - === Processing table: gold ===\n",
      "2025-05-24 17:47:58,200 - INFO - Reading from datalake.bronze.gold\n",
      "2025-05-24 17:47:58,486 - INFO - Table schema:\n",
      "2025-05-24 17:47:58,489 - INFO - Transforming data for table gold\n",
      "2025-05-24 17:47:58,531 - INFO - Cleaning and casting columns to appropriate types...\n",
      "2025-05-24 17:47:58,534 - INFO - Date column type: StringType()\n",
      "2025-05-24 17:47:58,534 - INFO - Converting date column to date type\n",
      "2025-05-24 17:47:58,576 - INFO - Cleaned string values in column 'price'\n",
      "2025-05-24 17:47:58,583 - INFO - Converted column 'price' to double type\n",
      "2025-05-24 17:47:58,599 - INFO - Cleaned string values in column 'open'\n",
      "2025-05-24 17:47:58,607 - INFO - Converted column 'open' to double type\n",
      "2025-05-24 17:47:58,626 - INFO - Cleaned string values in column 'high'\n",
      "2025-05-24 17:47:58,635 - INFO - Converted column 'high' to double type\n",
      "2025-05-24 17:47:58,653 - INFO - Cleaned string values in column 'low'\n",
      "2025-05-24 17:47:58,663 - INFO - Converted column 'low' to double type\n",
      "2025-05-24 17:47:58,665 - INFO - Filtering data from 1995-01-05 onward\n",
      "2025-05-24 17:47:58,674 - INFO - Processing gold with forward fill...\n",
      "2025-05-24 17:47:58,676 - INFO - Creating date range from 1995-01-05 to 2025-03-07...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- open: string (nullable = true)\n",
      " |-- high: string (nullable = true)\n",
      " |-- low: string (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- inserted: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 17:47:58,786 - INFO - Created continuous date range with 11020 dates\n",
      "2025-05-24 17:47:58,787 - INFO - Joining with full date range...\n",
      "2025-05-24 17:47:58,914 - INFO - Sample data after processing:\n",
      "[Stage 36:===============================================>       (81 + 13) / 94]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# A fatal error has been detected by the Java Runtime Environment:\n",
      "#\n",
      "#  SIGSEGV (0xb) at pc=0x00007f5279a99040, pid=65994, tid=66025\n",
      "#\n",
      "# JRE version: OpenJDK Runtime Environment (11.0.26+4) (build 11.0.26+4-post-Debian-1deb11u1)\n",
      "# Java VM: OpenJDK 64-Bit Server VM (11.0.26+4-post-Debian-1deb11u1, mixed mode, sharing, tiered, compressed oops, g1 gc, linux-amd64)\n",
      "# Problematic frame:\n",
      "# V  [libjvm.so+0xc35040]  ProtectionDomainEntry::object_no_keepalive()+0x0\n",
      "#\n",
      "# Core dump will be written. Default location: Core dumps may be processed with \"/usr/share/apport/apport -p%p -s%s -c%c -d%d -P%P -u%u -g%g -- %E\" (or dumping to /src/notebooks/Bao/core.65994)\n",
      "#\n",
      "# An error report file with more information is saved as:\n",
      "# /src/notebooks/Bao/hs_err_pid65994.log\n",
      "#\n",
      "# If you would like to submit a bug report, please visit:\n",
      "#   https://bugs.debian.org/openjdk-11\n",
      "#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 17:48:00,960 - INFO - Error while receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "2025-05-24 17:48:00,965 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:00,968 - ERROR - Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "2025-05-24 17:48:00,974 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:00,975 - ERROR - Error processing gold: An error occurred while calling o503.showString\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 21, in process_table\n",
      "    df_transformed = transform_data(spark, df, table_name)\n",
      "  File \"/tmp/ipykernel_65943/1993845443.py\", line 22, in transform_data\n",
      "    df = process_with_forward_fill(spark, df, table_name)\n",
      "  File \"/tmp/ipykernel_65943/75483517.py\", line 23, in process_with_forward_fill\n",
      "    result_df.orderBy(DATE_COLUMN).limit(3).show(truncate=False)\n",
      "  File \"/opt/spark/python/pyspark/sql/dataframe.py\", line 947, in show\n",
      "    print(self._show_string(n, truncate, vertical))\n",
      "  File \"/opt/spark/python/pyspark/sql/dataframe.py\", line 978, in _show_string\n",
      "    return self._jdf.showString(n, int_truncate, vertical)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o503.showString\n",
      "2025-05-24 17:48:00,978 - ERROR - Failed to process table gold\n",
      "2025-05-24 17:48:00,979 - INFO - === Processing table: inflation ===\n",
      "2025-05-24 17:48:00,980 - INFO - Reading from datalake.bronze.inflation\n",
      "2025-05-24 17:48:00,982 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:00,983 - ERROR - Error processing inflation: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:00,984 - ERROR - Failed to process table inflation\n",
      "2025-05-24 17:48:00,985 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:00,985 - INFO - === Processing table: interest ===\n",
      "2025-05-24 17:48:00,986 - INFO - Reading from datalake.bronze.interest\n",
      "2025-05-24 17:48:00,987 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:00,987 - ERROR - Error processing interest: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:00,988 - ERROR - Failed to process table interest\n",
      "2025-05-24 17:48:00,989 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:00,990 - INFO - === Processing table: msci_world ===\n",
      "2025-05-24 17:48:00,990 - INFO - Reading from datalake.bronze.msci_world\n",
      "2025-05-24 17:48:00,991 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:00,992 - ERROR - Error processing msci_world: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:00,992 - ERROR - Failed to process table msci_world\n",
      "2025-05-24 17:48:00,993 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:00,993 - INFO - === Processing table: nasdaq100 ===\n",
      "2025-05-24 17:48:00,994 - INFO - Reading from datalake.bronze.nasdaq100\n",
      "2025-05-24 17:48:00,995 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:00,996 - ERROR - Error processing nasdaq100: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:00,996 - ERROR - Failed to process table nasdaq100\n",
      "2025-05-24 17:48:00,997 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:00,998 - INFO - === Processing table: oil ===\n",
      "2025-05-24 17:48:00,998 - INFO - Reading from datalake.bronze.oil\n",
      "2025-05-24 17:48:00,999 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:00,999 - ERROR - Error processing oil: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:01,000 - ERROR - Failed to process table oil\n",
      "2025-05-24 17:48:01,000 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,001 - INFO - === Processing table: russell2000 ===\n",
      "2025-05-24 17:48:01,001 - INFO - Reading from datalake.bronze.russell2000\n",
      "2025-05-24 17:48:01,002 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,003 - ERROR - Error processing russell2000: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:01,003 - ERROR - Failed to process table russell2000\n",
      "2025-05-24 17:48:01,004 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,004 - INFO - === Processing table: s_p500 ===\n",
      "2025-05-24 17:48:01,005 - INFO - Reading from datalake.bronze.s_p500\n",
      "2025-05-24 17:48:01,006 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,006 - ERROR - Error processing s_p500: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:01,007 - ERROR - Failed to process table s_p500\n",
      "2025-05-24 17:48:01,007 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,007 - INFO - === Processing table: us_10_year_bond ===\n",
      "2025-05-24 17:48:01,008 - INFO - Reading from datalake.bronze.us_10_year_bond\n",
      "2025-05-24 17:48:01,009 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,009 - ERROR - Error processing us_10_year_bond: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:01,010 - ERROR - Failed to process table us_10_year_bond\n",
      "2025-05-24 17:48:01,010 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,011 - INFO - === Processing table: us_2_year_bond ===\n",
      "2025-05-24 17:48:01,011 - INFO - Reading from datalake.bronze.us_2_year_bond\n",
      "2025-05-24 17:48:01,013 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,014 - ERROR - Error processing us_2_year_bond: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:01,016 - ERROR - Failed to process table us_2_year_bond\n",
      "2025-05-24 17:48:01,017 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,017 - INFO - === Processing table: us_3_month_bond ===\n",
      "2025-05-24 17:48:01,018 - INFO - Reading from datalake.bronze.us_3_month_bond\n",
      "2025-05-24 17:48:01,019 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,019 - ERROR - Error processing us_3_month_bond: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:01,020 - ERROR - Failed to process table us_3_month_bond\n",
      "2025-05-24 17:48:01,021 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,022 - INFO - === Processing table: us_5_year_bond ===\n",
      "2025-05-24 17:48:01,023 - INFO - Reading from datalake.bronze.us_5_year_bond\n",
      "2025-05-24 17:48:01,024 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,025 - ERROR - Error processing us_5_year_bond: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:01,026 - ERROR - Failed to process table us_5_year_bond\n",
      "2025-05-24 17:48:01,027 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,028 - INFO - === Processing table: us_dollar ===\n",
      "2025-05-24 17:48:01,028 - INFO - Reading from datalake.bronze.us_dollar\n",
      "2025-05-24 17:48:01,029 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,030 - ERROR - Error processing us_dollar: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:01,032 - ERROR - Failed to process table us_dollar\n",
      "2025-05-24 17:48:01,033 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,033 - INFO - === Processing table: usd_vnd ===\n",
      "2025-05-24 17:48:01,034 - INFO - Reading from datalake.bronze.usd_vnd\n",
      "2025-05-24 17:48:01,035 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,036 - ERROR - Error processing usd_vnd: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:01,037 - ERROR - Failed to process table usd_vnd\n",
      "2025-05-24 17:48:01,037 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,038 - INFO - === Processing table: vn_10_year_bond ===\n",
      "2025-05-24 17:48:01,038 - INFO - Reading from datalake.bronze.vn_10_year_bond\n",
      "2025-05-24 17:48:01,039 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,040 - ERROR - Error processing vn_10_year_bond: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:01,040 - ERROR - Failed to process table vn_10_year_bond\n",
      "2025-05-24 17:48:01,041 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,041 - INFO - === Processing table: vn_2_year_bond ===\n",
      "2025-05-24 17:48:01,041 - INFO - Reading from datalake.bronze.vn_2_year_bond\n",
      "2025-05-24 17:48:01,042 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,043 - ERROR - Error processing vn_2_year_bond: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:01,044 - ERROR - Failed to process table vn_2_year_bond\n",
      "2025-05-24 17:48:01,044 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,044 - INFO - === Processing table: vn_5_year_bond ===\n",
      "2025-05-24 17:48:01,045 - INFO - Reading from datalake.bronze.vn_5_year_bond\n",
      "2025-05-24 17:48:01,046 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,046 - ERROR - Error processing vn_5_year_bond: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:01,047 - ERROR - Failed to process table vn_5_year_bond\n",
      "2025-05-24 17:48:01,047 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,048 - INFO - === Processing table: vnd_usd ===\n",
      "2025-05-24 17:48:01,048 - INFO - Reading from datalake.bronze.vnd_usd\n",
      "2025-05-24 17:48:01,049 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,049 - ERROR - Error processing vnd_usd: [Errno 111] Connection refused\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_65943/103345566.py\", line 11, in process_table\n",
      "    df = spark.table(source_table)\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 1667, in table\n",
      "    return DataFrame(self._jsparkSession.table(tableName), self)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1036, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 284, in _get_connection\n",
      "    connection = self._create_new_connection()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 291, in _create_new_connection\n",
      "    connection.connect_to_java_server()\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 438, in connect_to_java_server\n",
      "    self.socket.connect((self.java_address, self.java_port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "2025-05-24 17:48:01,050 - ERROR - Failed to process table vnd_usd\n",
      "2025-05-24 17:48:01,050 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,051 - INFO - === Job Summary ===\n",
      "2025-05-24 17:48:01,051 - INFO - Total tables found: 19\n",
      "2025-05-24 17:48:01,052 - INFO - Successfully processed: 1\n",
      "2025-05-24 17:48:01,052 - INFO - Failed: 18\n",
      "2025-05-24 17:48:01,053 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,318 - INFO - Error while receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "2025-05-24 17:48:01,338 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,415 - ERROR - Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "2025-05-24 17:48:01,472 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,542 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,595 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,664 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,712 - INFO - Closing down clientserver connection\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spark:\n\u001b[0;32m---> 34\u001b[0m         \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1796\u001b[0m, in \u001b[0;36mSparkSession.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;124;03mStop the underlying :class:`SparkContext`.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;124;03m>>> spark.stop()  # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SQLContext\n\u001b[0;32m-> 1796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[38;5;66;03m# We should clean the default session up. See SPARK-23228.\u001b[39;00m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/context.py:654\u001b[0m, in \u001b[0;36mSparkContext.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_jsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JError:\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;66;03m# Case: SPARK-18523\u001b[39;00m\n\u001b[1;32m    657\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to cleanly shutdown Spark JVM process.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m It is possible that the process has crashed,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m been killed or may also be in a zombie state.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    661\u001b[0m             \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[1;32m    662\u001b[0m         )\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 17:48:01,751 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,754 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,756 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,758 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,760 - INFO - Closing down clientserver connection\n",
      "2025-05-24 17:48:01,762 - INFO - Closing down clientserver connection\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Initialize Spark\n",
    "    spark = create_spark_session(\"Bronze_to_Silver_ETL\")\n",
    "    \n",
    "    # Ensure target namespace exists\n",
    "    create_namespace_if_not_exists(spark, TARGET_NAMESPACE)\n",
    "    \n",
    "    # List bronze tables to process\n",
    "    bronze_tables = list_tables(spark, SOURCE_NAMESPACE)\n",
    "        \n",
    "    # Process each table\n",
    "    processed_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    for table_name in bronze_tables:\n",
    "        try:\n",
    "            process_table(spark, table_name)\n",
    "            processed_count += 1\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process table {table_name}\")\n",
    "            failed_count += 1\n",
    "\n",
    "    # Summarize results\n",
    "    logger.info(\"=== Job Summary ===\")\n",
    "    logger.info(f\"Total tables found: {len(bronze_tables)}\")\n",
    "    logger.info(f\"Successfully processed: {processed_count}\")\n",
    "    logger.info(f\"Failed: {failed_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.critical(f\"Job failed with error: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    if spark:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54182550-a6ca-45c4-9df3-b7774dd11539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bbc896-53e6-4121-8bb1-764967787387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
