{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f1871ff5-d7c0-4fe8-9bbc-ed352f15ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame # type: ignore\n",
    "from pyspark.sql.functions import col, input_file_name, year, month, to_timestamp, lit, to_date # type: ignore\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(\"InsertRawToBronze\")\n",
    "\n",
    "# Constants\n",
    "SOURCE_NAMESPACE='test_append'\n",
    "BASE_RAW_DATA_DIR = \"/src/data/gold_test.csv\"\n",
    "TARGET_CATALOG = \"datalake\"\n",
    "TARGET_NAMESPACE = f\"{TARGET_CATALOG}.bronze\"\n",
    "DATE_FORMATS = [\n",
    "    'MM-dd-yy',\n",
    "    \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\", \n",
    "    \"yyyy-MM-dd'T'HH:mm:ssZ\", \n",
    "    'yyyy-MM-dd HH:mm:ss.SSS', \n",
    "    'yyyy-MM-dd HH:mm:ss', \n",
    "    'MM/dd/yyyy HH:mm:ss', \n",
    "    'MM/dd/yyyy', \n",
    "    'yyyy-MM-dd', \n",
    "    'dd/MM/yyyy', \n",
    "    'dd-MM-yyyy', \n",
    "    'MM-dd-yyyy',\n",
    "    'MMM d, yyyy',\n",
    "    'MMMM d, yyyy'\n",
    "]\n",
    "\n",
    "# Standard column mapping\n",
    "STANDARD_COLUMNS = {\n",
    "    'date': ['date', 'datetime', 'time'],\n",
    "    'price': ['price', 'close'],\n",
    "    'open': ['open', 'Open'],\n",
    "    'high': ['high', 'High'],\n",
    "    'low': ['low'],\n",
    "    'volume': ['vol', 'vol.', 'volume', 'Volume'],\n",
    "    'change': ['change', 'change %'],\n",
    "    'id': ['id'],\n",
    "    'adj': ['adj', 'Adj'],\n",
    "    'price_tip': ['close_tip'],  \n",
    "    'adj_price': ['adj_close', 'adj close']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "976ac2e6-6c66-4a0e-b803-b22c3223ee59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "        SparkSession.builder.appName(\"Test_read\")\n",
    "        .enableHiveSupport()\n",
    "        .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "        .config(\"spark.sql.avro.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {SOURCE_NAMESPACE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d9d1423c-bcfe-4f47-bb37-f248b81bd730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(name: str) -> str:\n",
    "    \"\"\"Clean folder or column names for SQL use\"\"\"\n",
    "    if not name:\n",
    "        return \"unnamed\"\n",
    "    \n",
    "    # Replace non-alphanumeric characters with underscore\n",
    "    cleaned = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", name)\n",
    "    # Clean up consecutive underscores\n",
    "    cleaned = re.sub(r\"_+\", \"_\", cleaned)\n",
    "    # Remove leading/trailing underscores\n",
    "    cleaned = cleaned.strip(\"_\")\n",
    "    # Convert to lowercase\n",
    "    cleaned = cleaned.lower()\n",
    "    # Add leading underscore if starts with digit\n",
    "    if cleaned and cleaned[0].isdigit():\n",
    "        cleaned = \"_\" + cleaned\n",
    "        \n",
    "    return cleaned\n",
    "\n",
    "def clean_column_names(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Clean all column names in DataFrame\"\"\"\n",
    "    original_columns = df.columns\n",
    "    new_columns = [clean_name(col) for col in original_columns]\n",
    "    \n",
    "    # Handle duplicate column names after cleaning\n",
    "    final_columns = []\n",
    "    seen = {}\n",
    "    for col in new_columns:\n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            final_columns.append(f\"{col}_{seen[col]}\")\n",
    "        else:\n",
    "            seen[col] = 0\n",
    "            final_columns.append(col)\n",
    "    \n",
    "    if original_columns != final_columns:\n",
    "        logger.warning(f\"Duplicate or invalid column names detected after cleaning. Renaming columns: {list(zip(original_columns, final_columns))}\")\n",
    "        return df.toDF(*final_columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "74c0b6ed-27d9-4c2c-a177-cd1a1962d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_schema(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Map columns to standard names and ensure consistent schema\"\"\"\n",
    "    # Log original columns for debugging\n",
    "    logger.info(f\"Original columns: {df.columns}\")\n",
    "    \n",
    "    # Create a mapping from actual column names to standard column names\n",
    "    column_mapping = {}\n",
    "    \n",
    "    # First, find all potential matches\n",
    "    potential_matches = {}\n",
    "    for std_col, possible_names in STANDARD_COLUMNS.items():\n",
    "        for actual_col in df.columns:\n",
    "            if actual_col.lower() in [name.lower() for name in possible_names]:\n",
    "                if std_col not in potential_matches:\n",
    "                    potential_matches[std_col] = []\n",
    "                potential_matches[std_col].append(actual_col)\n",
    "    \n",
    "    # Resolve the mappings, ensuring no duplicate target columns\n",
    "    used_columns = set()\n",
    "    for std_col, matched_columns in potential_matches.items():\n",
    "        if len(matched_columns) == 1:\n",
    "            # Only one match for this standard column\n",
    "            column_mapping[matched_columns[0]] = std_col\n",
    "            used_columns.add(matched_columns[0])\n",
    "        else:\n",
    "            # Multiple matches - need to create unique names\n",
    "            for idx, matched_col in enumerate(matched_columns):\n",
    "                if idx == 0:\n",
    "                    # First match gets the standard name\n",
    "                    column_mapping[matched_col] = std_col\n",
    "                else:\n",
    "                    # Subsequent matches get suffixed names\n",
    "                    column_mapping[matched_col] = f\"{std_col}_{idx}\"\n",
    "                used_columns.add(matched_col)\n",
    "    \n",
    "    # Include any remaining columns with their original names\n",
    "    for col in df.columns:\n",
    "        if col not in used_columns and col != \"_source_file\":\n",
    "            column_mapping[col] = col\n",
    "            \n",
    "    logger.info(f\"Column mapping: {column_mapping}\")\n",
    "    \n",
    "    # Create a standardized DataFrame\n",
    "    std_df = df\n",
    "    \n",
    "    # Rename columns according to mapping\n",
    "    for orig_col, std_col in column_mapping.items():\n",
    "        std_df = std_df.withColumnRenamed(orig_col, std_col)\n",
    "    \n",
    "    # Ensure source_file column exists\n",
    "    if \"_source_file\" in std_df.columns:\n",
    "        std_df = std_df.withColumnRenamed(\"_source_file\", \"source_file\")\n",
    "    \n",
    "    logger.info(f\"Standardized columns: {std_df.columns}\")\n",
    "    return std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e23ad30d-db90-476b-bde0-285e66e3f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date_column(df: DataFrame, date_col_name: str) -> DataFrame:\n",
    "    \"\"\"Try to parse the date column using multiple formats\"\"\"\n",
    "    logger.info(f\"Attempting to parse date column '{date_col_name}' using formats: {DATE_FORMATS}\")\n",
    "    \n",
    "    # Create a new DataFrame with a parsed date column\n",
    "    parsed_df = df\n",
    "    \n",
    "    # Try each date format in succession\n",
    "    for idx, date_format in enumerate(DATE_FORMATS):\n",
    "        try:\n",
    "            logger.info(f\"Trying date format #{idx+1}: {date_format}\")\n",
    "            # Create temporary column with this format\n",
    "            temp_df = parsed_df.withColumn(\n",
    "                \"parsed_date\", \n",
    "                to_timestamp(col(date_col_name), date_format)\n",
    "            )\n",
    "            \n",
    "            # Count non-null values with this format\n",
    "            valid_count = temp_df.filter(col(\"parsed_date\").isNotNull()).count()\n",
    "            logger.info(f\"Format {date_format} produced {valid_count} valid dates\")\n",
    "            \n",
    "            if valid_count > 0:\n",
    "                # Use this format since it worked for some values\n",
    "                parsed_df = temp_df\n",
    "                logger.info(f\"Using format '{date_format}' for date parsing\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Format '{date_format}' failed with error: {str(e)}\")\n",
    "    \n",
    "    return parsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "820cae27-e8e9-4b9c-b6fe-2bb3c0338496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_partition_columns(df: DataFrame) -> Tuple[DataFrame, List[str]]:\n",
    "    \"\"\"Add year and month partition columns from date column\"\"\"\n",
    "    logger.info(\"Attempting to add partition columns...\")\n",
    "    \n",
    "    # Find date column (case insensitive)\n",
    "    date_col_candidates = [c for c in df.columns if c.lower() == \"date\"]\n",
    "    \n",
    "    if not date_col_candidates:\n",
    "        logger.warning(\"No 'date' column found. Skipping partitioning.\")\n",
    "        return df, []\n",
    "    \n",
    "    date_col_name = date_col_candidates[0]\n",
    "    logger.info(f\"Found date column: '{date_col_name}'\")\n",
    "    \n",
    "    try:\n",
    "        # Parse the date column\n",
    "        df_with_parsed_date = parse_date_column(df, date_col_name)\n",
    "        \n",
    "        # Check if we successfully parsed any dates\n",
    "        if \"parsed_date\" not in df_with_parsed_date.columns:\n",
    "            logger.warning(\"Failed to parse date column with any format. Skipping partitioning.\")\n",
    "            return df, []\n",
    "            \n",
    "        # Add year and month columns from the parsed date\n",
    "        df_partitioned = (\n",
    "            df_with_parsed_date\n",
    "            .withColumn(\"year\", year(col(\"parsed_date\")))\n",
    "            .withColumn(\"month\", month(col(\"parsed_date\")))\n",
    "        )\n",
    "        \n",
    "        # Check if partition columns were successfully added\n",
    "        null_years = df_partitioned.filter(col(\"year\").isNull()).count()\n",
    "        if null_years > 0:\n",
    "            logger.warning(f\"{null_years} rows have NULL year values after date parsing\")\n",
    "        \n",
    "        logger.info(f\"Successfully added partition columns 'year', 'month' from '{date_col_name}'\")\n",
    "        \n",
    "        # Replace the original date column with the parsed_date and then drop the parsed_date column\n",
    "        df_partitioned = df_partitioned.drop(date_col_name).withColumnRenamed(\"parsed_date\", date_col_name)\n",
    "        \n",
    "        # Return the DataFrame with the partition columns\n",
    "        return df_partitioned, [\"year\", \"month\"]\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error adding partition columns: {str(e)}\")\n",
    "        logger.error(\"Continuing without partition columns\")\n",
    "        return df, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "404e7982-1783-4afc-a144-8a21c9b0191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_iceberg(df: DataFrame, table_name: str, partition_by: List[str]):\n",
    "    \"\"\"Write DataFrame to Iceberg table\"\"\"\n",
    "    logger.info(f\"Starting write operation to Iceberg table: {table_name}\")\n",
    "    logger.info(f\"Partitioning by: {partition_by}\")\n",
    "    logger.info(f\"Write mode: overwrite\")\n",
    "    logger.info(f\"DataFrame contains {df.count()} rows to write\")\n",
    "    \n",
    "    try:\n",
    "        writer = (\n",
    "            df.write\n",
    "            .format(\"iceberg\")\n",
    "            .mode(\"append\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "        )\n",
    "        \n",
    "        if partition_by and all(p in df.columns for p in partition_by):\n",
    "            # Ensure partition columns have valid data\n",
    "            null_partition_count = df.filter(\n",
    "                ' OR '.join([f\"{p} IS NULL\" for p in partition_by])\n",
    "            ).count()\n",
    "            \n",
    "            if null_partition_count > 0:\n",
    "                logger.warning(f\"{null_partition_count} rows have NULL partition values\")\n",
    "            \n",
    "            if null_partition_count < df.count():  # Only partition if some rows have valid values\n",
    "                writer = writer.partitionBy(*partition_by)\n",
    "                logger.info(f\"Partitioning by {partition_by}\")\n",
    "            else:\n",
    "                logger.warning(\"Skipping partitioning as all partition columns contain NULL values\")\n",
    "        else:\n",
    "            logger.warning(f\"Partition columns missing from DataFrame. Skipping partitioning.\")\n",
    "        \n",
    "        writer.saveAsTable(table_name)\n",
    "        logger.info(f\"Successfully wrote data to Iceberg table: {table_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write to Iceberg table {table_name}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c09b1652-8534-4631-9c19-a1704512a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"mode\", \"PERMISSIVE\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .csv(BASE_RAW_DATA_DIR)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cc42d805-be78-4447-8293-abb046ca9409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:03:00,630 - WARNING - Duplicate or invalid column names detected after cleaning. Renaming columns: [('_c0', 'c0'), ('Date', 'date'), ('Price', 'price'), ('Open', 'open'), ('High', 'high'), ('Low', 'low'), ('Vol.', 'vol'), ('Change %', 'change')]\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = clean_column_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d226e8a7-b40f-422c-bb9e-b8c5899a4060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:03:01,395 - INFO - Original columns: ['c0', 'date', 'price', 'open', 'high', 'low', 'vol', 'change']\n",
      "2025-05-20 16:03:01,396 - INFO - Column mapping: {'date': 'date', 'price': 'price', 'open': 'open', 'high': 'high', 'low': 'low', 'vol': 'volume', 'change': 'change', 'c0': 'c0'}\n",
      "2025-05-20 16:03:01,423 - INFO - Standardized columns: ['c0', 'date', 'price', 'open', 'high', 'low', 'volume', 'change']\n"
     ]
    }
   ],
   "source": [
    "df_standardized = standardize_schema(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a121bc35-0c5b-4939-b760-4eb147557166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:03:02,121 - INFO - Attempting to add partition columns...\n",
      "2025-05-20 16:03:02,124 - INFO - Found date column: 'date'\n",
      "2025-05-20 16:03:02,126 - INFO - Attempting to parse date column 'date' using formats: ['MM-dd-yy', \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\", \"yyyy-MM-dd'T'HH:mm:ssZ\", 'yyyy-MM-dd HH:mm:ss.SSS', 'yyyy-MM-dd HH:mm:ss', 'MM/dd/yyyy HH:mm:ss', 'MM/dd/yyyy', 'yyyy-MM-dd', 'dd/MM/yyyy', 'dd-MM-yyyy', 'MM-dd-yyyy', 'MMM d, yyyy', 'MMMM d, yyyy']\n",
      "2025-05-20 16:03:02,127 - INFO - Trying date format #1: MM-dd-yy\n",
      "2025-05-20 16:03:02,387 - WARNING - Format 'MM-dd-yy' failed with error: An error occurred while calling o590.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 174.0 failed 1 times, most recent failure: Lost task 0.0 in stage 174.0 (TID 123) (dca658d2bc53 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '03-08-2025' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '03-08-2025' could not be parsed, unparsed text found at index 8\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n",
      "\t... 19 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\n",
      "Fail to parse '03-08-2025' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n",
      "\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '03-08-2025' could not be parsed, unparsed text found at index 8\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2049)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n",
      "\t... 19 more\n",
      "\n",
      "2025-05-20 16:03:02,388 - INFO - Trying date format #2: yyyy-MM-dd'T'HH:mm:ss.SSSZ\n",
      "2025-05-20 16:03:02,497 - INFO - Format yyyy-MM-dd'T'HH:mm:ss.SSSZ produced 0 valid dates\n",
      "2025-05-20 16:03:02,499 - INFO - Trying date format #3: yyyy-MM-dd'T'HH:mm:ssZ\n",
      "2025-05-20 16:03:02,616 - INFO - Format yyyy-MM-dd'T'HH:mm:ssZ produced 0 valid dates\n",
      "2025-05-20 16:03:02,618 - INFO - Trying date format #4: yyyy-MM-dd HH:mm:ss.SSS\n",
      "2025-05-20 16:03:02,727 - INFO - Format yyyy-MM-dd HH:mm:ss.SSS produced 0 valid dates\n",
      "2025-05-20 16:03:02,730 - INFO - Trying date format #5: yyyy-MM-dd HH:mm:ss\n",
      "2025-05-20 16:03:02,884 - INFO - Format yyyy-MM-dd HH:mm:ss produced 0 valid dates\n",
      "2025-05-20 16:03:02,891 - INFO - Trying date format #6: MM/dd/yyyy HH:mm:ss\n",
      "2025-05-20 16:03:03,002 - INFO - Format MM/dd/yyyy HH:mm:ss produced 0 valid dates\n",
      "2025-05-20 16:03:03,005 - INFO - Trying date format #7: MM/dd/yyyy\n",
      "2025-05-20 16:03:03,112 - INFO - Format MM/dd/yyyy produced 0 valid dates\n",
      "2025-05-20 16:03:03,114 - INFO - Trying date format #8: yyyy-MM-dd\n",
      "2025-05-20 16:03:03,316 - INFO - Format yyyy-MM-dd produced 0 valid dates\n",
      "2025-05-20 16:03:03,317 - INFO - Trying date format #9: dd/MM/yyyy\n",
      "2025-05-20 16:03:03,445 - INFO - Format dd/MM/yyyy produced 0 valid dates\n",
      "2025-05-20 16:03:03,447 - INFO - Trying date format #10: dd-MM-yyyy\n",
      "2025-05-20 16:03:03,575 - INFO - Format dd-MM-yyyy produced 5 valid dates\n",
      "2025-05-20 16:03:03,577 - INFO - Using format 'dd-MM-yyyy' for date parsing\n",
      "2025-05-20 16:03:03,696 - INFO - Successfully added partition columns 'year', 'month' from 'date'\n"
     ]
    }
   ],
   "source": [
    "df_partitioned, partition_cols = add_partition_columns(df_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29bf63f9-56bc-4ac6-8d0f-37a7aaee3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_table_name = 'datalake.bronze.gold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2a1cb27c-f75f-43d2-8e88-325d2f740f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+--------+--------+-------------------+----+-----+-----------+\n",
      "| c0|   price|    open|    high|     low|               date|year|month|source_file|\n",
      "+---+--------+--------+--------+--------+-------------------+----+-----+-----------+\n",
      "|  1|2,926.11|2,916.80|2,931.26|2,906.30|2025-08-03 00:00:00|2025|    8|        abc|\n",
      "|  2|2,926.60|2,929.50|2,935.90|2,897.60|2025-09-03 00:00:00|2025|    9|        abc|\n",
      "|  3|2,926.00|2,929.00|2,941.30|2,903.40|2025-10-03 00:00:00|2025|   10|        abc|\n",
      "|  4|2,920.60|2,904.20|2,939.80|2,892.50|2025-11-03 00:00:00|2025|   11|        abc|\n",
      "|  5|2,901.10|2,872.00|2,906.40|2,866.30|2025-12-03 00:00:00|2025|   12|        abc|\n",
      "+---+--------+--------+--------+--------+-------------------+----+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_partitioned = df_partitioned.drop(col('volume'), col('change')).withColumn('source_file', lit('abc'))\n",
    "df_partitioned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e230517-3110-4e2e-b5df-5a3b56d0170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINANCIAL_COLUMNS = [\"price\", \"price_tip\", \"adj_price\", \"open\", \"high\", \"low\", \"volume\", \"change\"]\n",
    "for column in [c for c in df_partitioned.columns if c in FINANCIAL_COLUMNS]:\n",
    "    if column in df_partitioned.columns:\n",
    "        df_partitioned = df_partitioned.withColumn(column, col(column).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d261e022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- c0: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- source_file: string (nullable = false)\n",
      "\n",
      "Number of rows: 5\n"
     ]
    }
   ],
   "source": [
    "# Check schema before writing to ensure 'date' is properly typed\n",
    "df_partitioned.printSchema()\n",
    "print(f\"Number of rows: {df_partitioned.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e0f5e54-1416-4029-b82a-53d7728398ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:06:46,503 - INFO - Starting write operation to Iceberg table: datalake.bronze.gold\n",
      "2025-05-20 16:06:46,505 - INFO - Partitioning by: ['year', 'month']\n",
      "2025-05-20 16:06:46,505 - INFO - Write mode: overwrite\n",
      "2025-05-20 16:06:46,641 - INFO - DataFrame contains 5 rows to write\n",
      "2025-05-20 16:06:46,875 - INFO - Partitioning by ['year', 'month']\n",
      "2025-05-20 16:06:48,845 - INFO - Successfully wrote data to Iceberg table: datalake.bronze.gold\n"
     ]
    }
   ],
   "source": [
    "write_to_iceberg(df_partitioned, full_table_name, partition_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3093ec0e-1399-4356-831a-5dc4b3abe13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[c0: int, date: date, price: double, open: double, high: double, low: double, source_file: string, year: int, month: int]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = spark.table('datalake.bronze.gold')\n",
    "test.orderBy('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f4d785ed-8ff8-4030-a8b8-ff47932490e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(c0=10133, date=datetime.date(2002, 10, 1), price=322.2, open=324.3, high=325.0, low=321.6, source_file='file:///src/data/raw/Gold/Gold.csv', year=2002, month=10),\n",
       " Row(c0=10134, date=datetime.date(2002, 10, 2), price=322.8, open=322.1, high=323.6, low=320.8, source_file='file:///src/data/raw/Gold/Gold.csv', year=2002, month=10),\n",
       " Row(c0=10135, date=datetime.date(2002, 10, 3), price=322.4, open=322.8, high=325.5, low=322.0, source_file='file:///src/data/raw/Gold/Gold.csv', year=2002, month=10),\n",
       " Row(c0=10136, date=datetime.date(2002, 10, 4), price=323.3, open=322.1, high=323.6, low=320.0, source_file='file:///src/data/raw/Gold/Gold.csv', year=2002, month=10),\n",
       " Row(c0=10137, date=datetime.date(2002, 10, 5), price=323.3, open=322.1, high=323.6, low=320.0, source_file='file:///src/data/raw/Gold/Gold.csv', year=2002, month=10)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 👇 Lọc các dòng có date < 2025-08-03\n",
    "filtered_df = test.filter(col(\"date\") < to_timestamp(lit(\"2025-08-03\")))\n",
    "\n",
    "# 🖨️ In kết quả\n",
    "filtered_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c5629393-45f0-4508-bc28-200284ef9ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 15:25:41,681 - INFO - Attempting to add partition columns...\n",
      "2025-05-22 15:25:41,686 - INFO - Found date column: 'date'\n",
      "2025-05-22 15:25:41,687 - INFO - Attempting to parse date column 'date' using formats: ['MM-dd-yy', \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\", \"yyyy-MM-dd'T'HH:mm:ssZ\", 'yyyy-MM-dd HH:mm:ss.SSS', 'yyyy-MM-dd HH:mm:ss', 'MM/dd/yyyy HH:mm:ss', 'MM/dd/yyyy', 'yyyy-MM-dd', 'dd/MM/yyyy', 'dd-MM-yyyy', 'MM-dd-yyyy', 'MMM d, yyyy', 'MMMM d, yyyy']\n",
      "2025-05-22 15:25:41,688 - INFO - Trying date format #1: MM-dd-yy\n",
      "2025-05-22 15:25:43,241 - INFO - Format MM-dd-yy produced 11022 valid dates     \n",
      "2025-05-22 15:25:43,242 - INFO - Using format 'MM-dd-yy' for date parsing\n",
      "2025-05-22 15:25:44,409 - INFO - Successfully added partition columns 'year', 'month' from 'date'\n"
     ]
    }
   ],
   "source": [
    "df_partitioned, partition_cols = add_partition_columns(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "071974d9-d4a2-4336-ad86-c511f25bf4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 15:26:04,891 - INFO - Starting write operation to Iceberg table: datalake.bronze.gold\n",
      "2025-05-22 15:26:04,892 - INFO - Partitioning by: ['year', 'month']\n",
      "2025-05-22 15:26:04,893 - INFO - Write mode: overwrite\n",
      "2025-05-22 15:26:05,870 - INFO - DataFrame contains 11022 rows to write         \n",
      "2025-05-22 15:26:07,686 - INFO - Partitioning by ['year', 'month']              \n",
      "2025-05-22 15:26:16,682 - INFO - Successfully wrote data to Iceberg table: datalake.bronze.gold\n"
     ]
    }
   ],
   "source": [
    "full_table_name = 'datalake.bronze.gold'\n",
    "write_to_iceberg(df_partitioned, full_table_name, partition_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "119ba966-5c54-497e-babb-8b210c2c83b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+-----+--------------------+----+-----+-------------------+\n",
      "|   c0|price| open| high|  low|         source_file|year|month|               date|\n",
      "+-----+-----+-----+-----+-----+--------------------+----+-----+-------------------+\n",
      "|10133|322.2|324.3|325.0|321.6|file:///src/data/...|2002|   10|2002-10-01 00:00:00|\n",
      "|10134|322.8|322.1|323.6|320.8|file:///src/data/...|2002|   10|2002-10-02 00:00:00|\n",
      "|10135|322.4|322.8|325.5|322.0|file:///src/data/...|2002|   10|2002-10-03 00:00:00|\n",
      "|10136|323.3|322.1|323.6|320.0|file:///src/data/...|2002|   10|2002-10-04 00:00:00|\n",
      "|10137|323.3|322.1|323.6|320.0|file:///src/data/...|2002|   10|2002-10-05 00:00:00|\n",
      "+-----+-----+-----+-----+-----+--------------------+----+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_partitioned.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
